202310071026

Tags: #nlp

# Bag-of-Words

A vector is created for a vocabulary with the total count of the word at each index.  The vector's indices are fixed to specific words in the vocabulary.

It could be thought of as an [[embedding]].

The total count of word $j$ in the vector $x$ (i.e. $x_j$) or, 1 for present, 0 for not present.

![[Pasted image 20231007103929.png]]

---
# References
- Eisenstein, pg 16