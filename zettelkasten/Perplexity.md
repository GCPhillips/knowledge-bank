202310111905

Tags: #nlp

# Perplexity
A measurement that is used to show how well a [[Language Model]] predicts a given sequence.

Lower perplexity $\rightarrow$ more accurate in predictions
Higher perplexity $\rightarrow$ less accurate in predictions

$$
perplexity(W) = P(w_1, w_2, \dots, w_n)^{\frac{-1}{n}}
$$
where $W$ is the test data.

---
# References
