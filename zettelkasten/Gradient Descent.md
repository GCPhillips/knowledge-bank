202401150710

Tags: #optimization #machinelearning 

# Gradient Descent
A method for optimizing a linear function.

#### Steps:
1.) Initialize $\beta_0$
2.) GD Update:
$$
\beta_{t+1} = \beta_t + \eta \nabla f(\beta_t)
$$
where $\beta_t$ is where we are after $t$ iterations, $\eta$ is the step size, and $\nabla f(...)$ is the gradient of function $f$.

---
# References
