202312061549

Tags: #reinforcementlearning 

# Reward Hacking
When the [[Agent]] finds a way to optimize the reward with unintended results.

For example, a robot that is trying to learn to jump might have a [[Reward]] for jumping that is proportional to how long they're off the ground.  The robot might learn to climb a nearby ladder or pole and stay off the ground.

---
# References
